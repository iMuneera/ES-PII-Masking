{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('Fake_data.csv')\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "from spacy.tokens import Span\n",
    "\n",
    "# Function to scan text file for sensitive data\n",
    "def scan_text_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    # Define a regex pattern for phone numbers\n",
    "    PHONE_PATTERN = r\"\\(\\d{3}\\) \\d{3}-\\d{4}\"\n",
    "    # Define a regex pattern for emails\n",
    "    EMAIL_PATTERN = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
    "    # Define a regex pattern for employee IDs\n",
    "    EMPLOYEE_ID_PATTERN = r\"EMP-\\d{6}\"\n",
    "    # Define a regex pattern for IP addresses\n",
    "    IP_ADDRESS_PATTERN = r\"\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b\"\n",
    "    doc = nlp(text)\n",
    "    sensitive_data = []\n",
    "\n",
    "       # Detect phone numbers\n",
    "    phone_matches = re.finditer(PHONE_PATTERN, text)\n",
    "    for match in phone_matches:\n",
    "        phone_number = match.group()\n",
    "        sensitive_data.append((phone_number, \"PHONE_NUMBER\"))\n",
    "        print(phone_number, \"PHONE_NUMBER\")\n",
    "\n",
    "    # Detect emails\n",
    "    email_matches = re.finditer(EMAIL_PATTERN, text)\n",
    "    for match in email_matches:\n",
    "        email = match.group()\n",
    "        sensitive_data.append((email, \"EMAIL\"))\n",
    "        print(email, \"EMAIL\")\n",
    "        # Detect employee IDs\n",
    "    employee_id_matches = re.finditer(EMPLOYEE_ID_PATTERN, text)\n",
    "    for match in employee_id_matches:\n",
    "            employee_id = match.group()\n",
    "            sensitive_data.append((employee_id, \"EMPLOYEE_ID\"))\n",
    "            print(employee_id, \"EMPLOYEE_ID\")\n",
    "\n",
    "        # Detect IP addresses\n",
    "    ip_address_matches = re.finditer(IP_ADDRESS_PATTERN, text)\n",
    "    for match in ip_address_matches:\n",
    "            ip_address = match.group()\n",
    "            sensitive_data.append((ip_address, \"IP_ADDRESS\"))\n",
    "            print(ip_address, \"IP_ADDRESS\")\n",
    "\n",
    "    # Detect other named entities\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, ent.label_)\n",
    "        if ent.label_ in [\"PERSON\", \"EMAIL\", \"GPE\"]:\n",
    "            sensitive_data.append((ent.text, ent.label_))\n",
    "    \n",
    "    return sensitive_data\n",
    "# Load and scan the text file\n",
    "file_path = 'train.txt'\n",
    "sensitive_data = scan_text_file(file_path)\n",
    "print(sensitive_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and scan the text file for sensitive data\n",
    "file_path = 'train.txt'\n",
    "sensitive_data = scan_text_file(file_path)\n",
    "\n",
    "# Function to mask sensitive data\n",
    "def mask_sensitive_data(text):\n",
    "    for data, label in sensitive_data:\n",
    "        text = text.replace(data, '*' * len(data))\n",
    "    return text\n",
    "\n",
    "# Read the file content\n",
    "with open(file_path, 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Mask sensitive data in the text\n",
    "masked_text = mask_sensitive_data(text)\n",
    "\n",
    "# Save the masked text to a new file\n",
    "with open('checked.txt', 'w') as file:\n",
    "    file.write(masked_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Function to extract text from specific pages of a PDF\n",
    "def extract_text_from_pages(pdf_path, page_numbers):\n",
    "    # Open the PDF file\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    extracted_text = \"\"\n",
    "\n",
    "    # Iterate through the specified page numbers\n",
    "    for page_num in page_numbers:\n",
    "        # Load the page\n",
    "        page = pdf_document.load_page(page_num - 1)  # page numbers are 0-based in PyMuPDF\n",
    "        # Extract text from the page\n",
    "        extracted_text += page.get_text()\n",
    "\n",
    "    return extracted_text, pdf_document.page_count\n",
    "\n",
    "# Specify the path to the PDF file and the pages to extract\n",
    "pdf_path = 'again.pdf'\n",
    "# Extract text from all pages and count the number of pages\n",
    "text, num_pages = extract_text_from_pages(pdf_path, range(1, fitz.open(pdf_path).page_count + 1))\n",
    "print(f\"Number of pages in the PDF: {num_pages}\")\n",
    "\n",
    "# Remove white spaces and tabs from the extracted text\n",
    "text = ' '.join(text.split())\n",
    "print(text)\n",
    "\n",
    "# Count the number of tokens in the text variable\n",
    "doc = nlp(text)\n",
    "\n",
    "num_tokens = len(doc)\n",
    "print(f\"Number of tokens in the text: {num_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "\n",
    "# Load the NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def mask_sensitive_data(text):\n",
    "    # Define regex patterns\n",
    " \n",
    "    PHONE_PATTERN = r\"\\+973 \\d{8}\"\n",
    "    EMAIL_PATTERN = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
    "    EMPLOYEE_ID_PATTERN = r\"EMP-\\d{6}\"\n",
    "    IP_ADDRESS_PATTERN = r\"\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b\"\n",
    "    CREDIT_CARD_PATTERN = r\"\\b(?:\\d[ -]*?){13,16}\\b\"\n",
    "\n",
    "    # Masking function\n",
    "    def mask_match(match):\n",
    "        return \"*\" * len(match.group())\n",
    "\n",
    "    # Mask sensitive data using regex\n",
    "    masked_text = re.sub(PHONE_PATTERN, mask_match, text)\n",
    "    masked_text = re.sub(EMAIL_PATTERN, mask_match, masked_text)\n",
    "    masked_text = re.sub(EMPLOYEE_ID_PATTERN, mask_match, masked_text)\n",
    "    masked_text = re.sub(IP_ADDRESS_PATTERN, mask_match, masked_text)\n",
    "    masked_text = re.sub(CREDIT_CARD_PATTERN, mask_match, masked_text)\n",
    "\n",
    "    # Process with NLP for named entities\n",
    "    doc = nlp(text)\n",
    "    # Print all entity labels in the spaCy library\n",
    "    for label in nlp.get_pipe(\"ner\").labels:\n",
    "        print(label)\n",
    "    for ent in doc.ents:\n",
    "      \n",
    "        print(ent.text, ent.label_)\n",
    "        if ent.label_ in [\"PERSON\", \"EMAIL\", \"GPE\",\"LOC\",\"FAC\"]:  # Mask names, emails, and locations\n",
    "            print(ent.text, ent.label_)\n",
    "            masked_text = masked_text.replace(ent.text, \"*\" * len(ent.text))\n",
    "\n",
    "    return masked_text\n",
    "def save_as_pdf(text, filename=\"masked_text.pdf\"):\n",
    "    # Create a PDF\n",
    "    pdf = canvas.Canvas(filename, pagesize=letter)\n",
    "    pdf.setFont(\"Helvetica\", 12)\n",
    "\n",
    "    # Split text into lines for PDF formatting\n",
    "    lines = text.split(\"\\n\")\n",
    "    y_position = 750  # Starting position on page\n",
    "\n",
    "    for line in lines:\n",
    "        # Split long lines to fit within the page width\n",
    "        while len(line) > 90:\n",
    "            pdf.drawString(50, y_position, line[:90])\n",
    "            line = line[90:]\n",
    "            y_position -= 20\n",
    "            if y_position < 50:  \n",
    "                pdf.showPage()\n",
    "                pdf.setFont(\"Helvetica\", 12)\n",
    "                y_position = 750\n",
    "\n",
    "        # Add new line after each period\n",
    "        sentences = line.split('. ')\n",
    "        for sentence in sentences:\n",
    "            pdf.drawString(50, y_position, sentence.strip() + '.')\n",
    "            y_position -= 20\n",
    "            if y_position < 50:  \n",
    "                pdf.showPage()\n",
    "                pdf.setFont(\"Helvetica\", 12)\n",
    "                y_position = 750\n",
    "\n",
    "        if y_position < 50:  \n",
    "            pdf.showPage()\n",
    "            pdf.setFont(\"Helvetica\", 12)\n",
    "            y_position = 750\n",
    "\n",
    "    pdf.save()\n",
    "    print(f\"PDF saved as {filename}\")\n",
    "\n",
    "\n",
    "# Mask data and save it as a PDF\n",
    "masked_text = mask_sensitive_data(text)\n",
    "save_as_pdf(masked_text)\n",
    "\n",
    "# Print masked text (optional)\n",
    "print(masked_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_url_safety(url, api_key):\n",
    "    # Encode the URL in base64 format (as required by VirusTotal)\n",
    "    url_id = base64.urlsafe_b64encode(url.encode()).decode().strip(\"=\")\n",
    "    vt_url = f\"https://www.virustotal.com/api/v3/urls/{url_id}\"\n",
    "    headers = {\"x-apikey\": api_key}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(vt_url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            malicious_count = data.get(\"data\", {}).get(\"attributes\", {}).get(\"last_analysis_stats\", {}).get(\"malicious\", 0)\n",
    "            return \"Malicious\" if malicious_count > 0 else \"Safe\"\n",
    "        else:\n",
    "            return f\"Error: {response.status_code} {response.reason}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found URL: http://testsafebrowsing.appspot.com/s/phishing.html\n",
      "URL Safety Check Result: Malicious\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import requests\n",
    "def main():\n",
    "\n",
    "   \n",
    "    \n",
    "        # Check for URLs\n",
    "    urls =\"http://testsafebrowsing.appspot.com/s/phishing.html\"\n",
    "    print(f\"Found URL: {urls}\")\n",
    "    result = check_url_safety(urls, api_key)\n",
    "    print(f\"URL Safety Check Result: {result}\")\n",
    "      \n",
    "\n",
    "        # Check for blacklisted words\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔒 Encrypted File Content (Hex): 674141414141426e7361727966677337796e4a47562d4943584634724f73775a6a4c777771346e6b534155714c633536333578717a43684f7a2d577963425037784239414a53454b4d5635656e354a586f4b63682d375f546a773448303567546b4852773674674e7949795543645f487371624a51424e4f6f6d786a49475671547267546a4a36304337323057506d754a7766686e70714a5f2d45747842456e6e773d3d\n",
      "\n",
      "🔓 Decrypted File Content: Sensitive File Content: User ID 123456, SSN: 987-65-4321\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tenseal as ts\n",
    "from cryptography.fernet import Fernet\n",
    "\n",
    "KEYS_DIR = \"ProcessedFiles/keys\"\n",
    "ENCRYPTED_DIR = \"ProcessedFiles/encrypted\"\n",
    "\n",
    "# ----- AES-256 ENCRYPTION -----\n",
    "def generate_key():\n",
    "    \"\"\"Generate and return an AES encryption key.\"\"\"\n",
    "    return Fernet.generate_key()\n",
    "\n",
    "def save_key(key, file_name):\n",
    "    \"\"\"Save encryption key.\"\"\"\n",
    "    if not os.path.exists(KEYS_DIR):\n",
    "        os.makedirs(KEYS_DIR)\n",
    "    key_path = os.path.join(KEYS_DIR, file_name + \".key\")\n",
    "    with open(key_path, \"wb\") as kf:\n",
    "        kf.write(key)\n",
    "\n",
    "def encrypt_data(data, key):\n",
    "    \"\"\"Encrypt data using AES-256.\"\"\"\n",
    "    cipher = Fernet(key)\n",
    "    return cipher.encrypt(data)\n",
    "\n",
    "def decrypt_data(encrypted_data, key):\n",
    "    \"\"\"Decrypt data using AES-256.\"\"\"\n",
    "    cipher = Fernet(key)\n",
    "    return cipher.decrypt(encrypted_data)\n",
    "\n",
    "def save_encrypted_file(encrypted_data, file_name):\n",
    "    \"\"\"Save the encrypted file to disk.\"\"\"\n",
    "    if not os.path.exists(ENCRYPTED_DIR):\n",
    "        os.makedirs(ENCRYPTED_DIR)\n",
    "    file_path = os.path.join(ENCRYPTED_DIR, file_name)\n",
    "    with open(file_path, \"wb\") as ef:\n",
    "        ef.write(encrypted_data)\n",
    "\n",
    "def read_encrypted_file(file_name):\n",
    "    \"\"\"Read and return the encrypted file contents.\"\"\"\n",
    "    file_path = os.path.join(ENCRYPTED_DIR, file_name)\n",
    "    with open(file_path, \"rb\") as ef:\n",
    "        return ef.read()\n",
    "\n",
    "# ----- TEST FILE ENCRYPTION -----\n",
    "if __name__ == \"__main__\":\n",
    "    aes_key = generate_key()\n",
    "    save_key(aes_key, \"file_key\")\n",
    "\n",
    "    # Original Data\n",
    "    data = b\"Sensitive File Content: User ID 123456, SSN: 987-65-4321\"\n",
    "\n",
    "    # Encrypt and Save\n",
    "    encrypted_file_data = encrypt_data(data, aes_key)\n",
    "    save_encrypted_file(encrypted_file_data, \"encrypted_file.dat\")\n",
    "\n",
    "    # Read and Display Encrypted Content\n",
    "    encrypted_content = read_encrypted_file(\"encrypted_file.dat\")\n",
    "    print(\"\\n🔒 Encrypted File Content (Hex):\", encrypted_content.hex())\n",
    "\n",
    "    # Decrypt and Display Original Content\n",
    "    decrypted_file_data = decrypt_data(encrypted_content, aes_key)\n",
    "    print(\"\\n🔓 Decrypted File Content:\", decrypted_file_data.decode())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = [\n",
    "    (\"My Bahraini number is +973 3345 6789.\", {\"entities\": [(21, 35, \"PHONE_NUMBER\")]}),\n",
    "    (\"You can reach me at +973 1245 6789.\", {\"entities\": [(19, 33, \"PHONE_NUMBER\")]}),\n",
    "    (\"Call me on +973 9988 1122.\", {\"entities\": [(15, 29, \"PHONE_NUMBER\")]}),\n",
    "    (\"My contact number is +973 3XXX XXXX.\", {\"entities\": [(24, 38, \"PHONE_NUMBER\")]}),\n",
    "    (\"For emergencies, call +973 8000 9999.\", {\"entities\": [(30, 44, \"PHONE_NUMBER\")]}),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xxlm5\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"My Bahraini number is +973 3345 6789.\" with entities \"[(21, 35, 'PHONE_NUMBER')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\xxlm5\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"You can reach me at +973 1245 6789.\" with entities \"[(19, 33, 'PHONE_NUMBER')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\xxlm5\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Call me on +973 9988 1122.\" with entities \"[(15, 29, 'PHONE_NUMBER')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\xxlm5\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"My contact number is +973 3XXX XXXX.\" with entities \"[(24, 38, 'PHONE_NUMBER')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\xxlm5\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"For emergencies, call +973 8000 9999.\" with entities \"[(30, 44, 'PHONE_NUMBER')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "from spacy.training import offsets_to_biluo_tags\n",
    "\n",
    "# Load the pre-trained model (you can use a smaller one like 'en_core_web_sm')\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Add a new entity label for phone numbers if it's not already in the pipeline\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe(\"ner\")\n",
    "    nlp.add_pipe(ner, last=True)\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "ner.add_label(\"PHONE_NUMBER\")\n",
    "\n",
    "# Prepare the optimizer and begin training\n",
    "optimizer = nlp.resume_training()\n",
    "\n",
    "# Fine-tune the model on the provided training data\n",
    "for epoch in range(100):\n",
    "    print(f\"Epoch: {epoch + 1}\")\n",
    "    for text, annotations in TRAIN_DATA:\n",
    "        doc = nlp.make_doc(text)\n",
    "        tags = offsets_to_biluo_tags(doc, annotations[\"entities\"])\n",
    "        example = Example.from_dict(doc, {\"entities\": annotations[\"entities\"], \"tags\": tags})\n",
    "        example = Example.from_dict(doc, {\"entities\": annotations[\"entities\"]})\n",
    "        nlp.update([example], sgd=optimizer)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "nlp.to_disk(\"bahraini_phone_number_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xxlm5\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"My Bahraini number is +973 3345 6789.\" with entities \"[(21, 35, 'PHONE_NUMBER')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\xxlm5\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"You can reach me at +973 1245 6789.\" with entities \"[(19, 33, 'PHONE_NUMBER')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\xxlm5\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Call me on +973 9988 1122.\" with entities \"[(15, 29, 'PHONE_NUMBER')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\xxlm5\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"My contact number is +973 3XXX XXXX.\" with entities \"[(24, 38, 'PHONE_NUMBER')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\xxlm5\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"For emergencies, call +973 8000 9999.\" with entities \"[(30, 44, 'PHONE_NUMBER')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Epoch: 11\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20\n",
      "Epoch: 21\n",
      "Epoch: 22\n",
      "Epoch: 23\n",
      "Epoch: 24\n",
      "Epoch: 25\n",
      "Epoch: 26\n",
      "Epoch: 27\n",
      "Epoch: 28\n",
      "Epoch: 29\n",
      "Epoch: 30\n",
      "Epoch: 31\n",
      "Epoch: 32\n",
      "Epoch: 33\n",
      "Epoch: 34\n",
      "Epoch: 35\n",
      "Epoch: 36\n",
      "Epoch: 37\n",
      "Epoch: 38\n",
      "Epoch: 39\n",
      "Epoch: 40\n",
      "Epoch: 41\n",
      "Epoch: 42\n",
      "Epoch: 43\n",
      "Epoch: 44\n",
      "Epoch: 45\n",
      "Epoch: 46\n",
      "Epoch: 47\n",
      "Epoch: 48\n",
      "Epoch: 49\n",
      "Epoch: 50\n",
      "Epoch: 51\n",
      "Epoch: 52\n",
      "Epoch: 53\n",
      "Epoch: 54\n",
      "Epoch: 55\n",
      "Epoch: 56\n",
      "Epoch: 57\n",
      "Epoch: 58\n",
      "Epoch: 59\n",
      "Epoch: 60\n",
      "Epoch: 61\n",
      "Epoch: 62\n",
      "Epoch: 63\n",
      "Epoch: 64\n",
      "Epoch: 65\n",
      "Epoch: 66\n",
      "Epoch: 67\n",
      "Epoch: 68\n",
      "Epoch: 69\n",
      "Epoch: 70\n",
      "Epoch: 71\n",
      "Epoch: 72\n",
      "Epoch: 73\n",
      "Epoch: 74\n",
      "Epoch: 75\n",
      "Epoch: 76\n",
      "Epoch: 77\n",
      "Epoch: 78\n",
      "Epoch: 79\n",
      "Epoch: 80\n",
      "Epoch: 81\n",
      "Epoch: 82\n",
      "Epoch: 83\n",
      "Epoch: 84\n",
      "Epoch: 85\n",
      "Epoch: 86\n",
      "Epoch: 87\n",
      "Epoch: 88\n",
      "Epoch: 89\n",
      "Epoch: 90\n",
      "Epoch: 91\n",
      "Epoch: 92\n",
      "Epoch: 93\n",
      "Epoch: 94\n",
      "Epoch: 95\n",
      "Epoch: 96\n",
      "Epoch: 97\n",
      "Epoch: 98\n",
      "Epoch: 99\n",
      "Epoch: 100\n",
      "Epoch: 101\n",
      "Epoch: 102\n",
      "Epoch: 103\n",
      "Epoch: 104\n",
      "Epoch: 105\n",
      "Epoch: 106\n",
      "Epoch: 107\n",
      "Epoch: 108\n",
      "Epoch: 109\n",
      "Epoch: 110\n",
      "Epoch: 111\n",
      "Epoch: 112\n",
      "Epoch: 113\n",
      "Epoch: 114\n",
      "Epoch: 115\n",
      "Epoch: 116\n",
      "Epoch: 117\n",
      "Epoch: 118\n",
      "Epoch: 119\n",
      "Epoch: 120\n",
      "Epoch: 121\n",
      "Epoch: 122\n",
      "Epoch: 123\n",
      "Epoch: 124\n",
      "Epoch: 125\n",
      "Epoch: 126\n",
      "Epoch: 127\n",
      "Epoch: 128\n",
      "Epoch: 129\n",
      "Epoch: 130\n",
      "Epoch: 131\n",
      "Epoch: 132\n",
      "Epoch: 133\n",
      "Epoch: 134\n",
      "Epoch: 135\n",
      "Epoch: 136\n",
      "Epoch: 137\n",
      "Epoch: 138\n",
      "Epoch: 139\n",
      "Epoch: 140\n",
      "Epoch: 141\n",
      "Epoch: 142\n",
      "Epoch: 143\n",
      "Epoch: 144\n",
      "Epoch: 145\n",
      "Epoch: 146\n",
      "Epoch: 147\n",
      "Epoch: 148\n",
      "Epoch: 149\n",
      "Epoch: 150\n",
      "Epoch: 151\n",
      "Epoch: 152\n",
      "Epoch: 153\n",
      "Epoch: 154\n",
      "Epoch: 155\n",
      "Epoch: 156\n",
      "Epoch: 157\n",
      "Epoch: 158\n",
      "Epoch: 159\n",
      "Epoch: 160\n",
      "Epoch: 161\n",
      "Epoch: 162\n",
      "Epoch: 163\n",
      "Epoch: 164\n",
      "Epoch: 165\n",
      "Epoch: 166\n",
      "Epoch: 167\n",
      "Epoch: 168\n",
      "Epoch: 169\n",
      "Epoch: 170\n",
      "Epoch: 171\n",
      "Epoch: 172\n",
      "Epoch: 173\n",
      "Epoch: 174\n",
      "Epoch: 175\n",
      "Epoch: 176\n",
      "Epoch: 177\n",
      "Epoch: 178\n",
      "Epoch: 179\n",
      "Epoch: 180\n",
      "Epoch: 181\n",
      "Epoch: 182\n",
      "Epoch: 183\n",
      "Epoch: 184\n",
      "Epoch: 185\n",
      "Epoch: 186\n",
      "Epoch: 187\n",
      "Epoch: 188\n",
      "Epoch: 189\n",
      "Epoch: 190\n",
      "Epoch: 191\n",
      "Epoch: 192\n",
      "Epoch: 193\n",
      "Epoch: 194\n",
      "Epoch: 195\n",
      "Epoch: 196\n",
      "Epoch: 197\n",
      "Epoch: 198\n",
      "Epoch: 199\n",
      "Epoch: 200\n",
      "Detected entity: +973 3345 6789 with label: PHONE_NUMBER\n",
      "Phone number detected: +973 3345 6789\n"
     ]
    }
   ],
   "source": [
    "# Additional training data\n",
    "MORE_TRAIN_DATA = [\n",
    "    (\"Contact us at +973 4455 6677.\", {\"entities\": [(14, 28, \"PHONE_NUMBER\")]}),\n",
    "    (\"Emergency number: +973 5566 7788.\", {\"entities\": [(18, 32, \"PHONE_NUMBER\")]}),\n",
    "    (\"Reach out at +973 6677 8899.\", {\"entities\": [(13, 27, \"PHONE_NUMBER\")]}),\n",
    "    (\"Call +973 7788 9900 for support.\", {\"entities\": [(5, 19, \"PHONE_NUMBER\")]}),\n",
    "    (\"Phone: +973 8899 0011.\", {\"entities\": [(7, 21, \"PHONE_NUMBER\")]}),\n",
    "]\n",
    "\n",
    "# Combine the original and additional training data\n",
    "TRAIN_DATA.extend(MORE_TRAIN_DATA)\n",
    "\n",
    "# Fine-tune the model on the combined training data\n",
    "for epoch in range(200):  # Increase the number of epochs\n",
    "    print(f\"Epoch: {epoch + 1}\")\n",
    "    for text, annotations in TRAIN_DATA:\n",
    "        doc = nlp.make_doc(text)\n",
    "        tags = offsets_to_biluo_tags(doc, annotations[\"entities\"])\n",
    "        example = Example.from_dict(doc, {\"entities\": annotations[\"entities\"], \"tags\": tags})\n",
    "        example = Example.from_dict(doc, {\"entities\": annotations[\"entities\"]})\n",
    "        nlp.update([example], sgd=optimizer)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "nlp.to_disk(\"bahraini_phone_number_model\")\n",
    "\n",
    "# Load the fine-tuned model\n",
    "nlp = spacy.load(\"bahraini_phone_number_model\")\n",
    "\n",
    "# Test the model on a new sentence\n",
    "doc = nlp(\"For assistance, contact +973 3345 6789 or +973 8000 5555.\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"Detected entity: {ent.text} with label: {ent.label_}\")\n",
    "    if ent.label_ == \"PHONE_NUMBER\":\n",
    "        print(f\"Phone number detected: {ent.text}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
